# 项目日记

 ```
    项目开始了，目的是通过状态保存，为模型保存记忆。我决定使用encoder-decoder的架构，在中间插入memory模块。我只完成过文本分类项目，并不了解关于语言生成模型的任何细节，之后要开始找训练数据，以及项目的模块化设计。
                                    4.14
                                    wqws
```
——
 ```
    为了能让模型学会怎么保存并使用自己的记忆，我必须让会话成为整个训练数据单元，而不是把会话中的每一个问答回合分开，单独训练。为此，训练循环还需要嵌套一层。
                                    4.20
                                    wqws
```
——
```
    毫无疑问，使用LSTM是保存状态的最佳方案，我是说我已经没有别的方案可以选择了，难道要去用rnn吗。在此之前没有人做过这件事情，我只能论证出最优的并且可实现的方案。
                                    4.24
                                    wqws
```
——
```
    encoder的输出为[b_s,token,dim],LSTM的输出虽然也是[b_s,token,dim]，但在以token为时间步时，常常截取[b_s,-1,dim]为输出，但现在的时间步是对话回合。我不想管那么多，只要能保存状态，其他的交给梯度下降好了。我决定直接使用完整的LSTM输出
                                    4.25
                                    wqws
```
——
```
    我打算把encoder的输出x，将它输入到memory中得到y1和y2，最的输出 memory_output = y1 * (y2+ x)，这算是伪残差链接吗?是的，我并不信任LSTM，所以我不能让memory全盘接管encoder输出的数据，必须要有残差连接，即便这不是正式的残差连接。
                                    4.26
                                    wqws
```
——
```
    模型的构建工作完成了，但开始训练时发生严重报错。好像是因为：
    “LSTM的隐藏状态会经过多次反向传播，当进行第一次反向传播时，计算图会释放，导致之后再此传播时计算图在此处消失。”
    我难以描述这个错误。
    我尝试截断隐藏状态的梯度传递，之后就没有报错了。
                                    4.26
                                    wqws
```
——
```
    出现了很多Nan，我尝试插入很多归一化，限制参数初始化在(-5，5)之间，使位置编码的标准差为0.2，增加了很多防止梯度爆炸的手段，但没有任何效果。
                                    4.27
                                    wqws
```
——
```
    我把测试代码插入模型的每一处，最后发现是全填充的句子导致交叉熵损失函数出现x/0，从而出现Nan，解决了这个问题。模型能够开始训练，loss稳定下降。
                                    4.31
                                    wqws
```
——
```
    我测试了一下模型，结果令人震惊，模型只会输出类似于“，。，，。。，，。。。。。，”的序列。
                                    5.2
                                    wqws
```
——
```
    我怀疑是memory的问题，我把计算公式修改为   
    memory_output = y1 * (y2+ x) + x，
    增加了更强制的残差连接。并且将训练数据量降低至1024，模型居然真能生成一些由词语构成的序列，好吧至少也算不错了。
    也因为这个，我能找到模型回答间存在的关联词，进而论证了memory的记忆功能。
                                    5.7
                                    wqws
```
——
```
    我继续增加训练数据，没想到又出现了大量标点符号情况，经过我的反复测试:
    loss>2  模型生成无意义词语序列
    17.5<loss<1.8   模型生成有关联的词语序列
    1.3<loss<1.7    模型生成大量重复词语
    loss<1.3    模型生成大量标点符号

    我能能通过 学习率，训练数据量，训练轮次，模型参数初始化 来控制最终loss，最终控制模型生成效果，最终论证memory能否起作用。
                                    5.8
                                    wqws
```
——
```
    经过很多测试，我尝试让模型的损失达到1.6，并搭配重复惩罚，这种尝试就像走钢丝一样，到底也没有太多成效。
                                    5.9
                                    wqws
```
——
```
    我居然发现，encoder没有在工作？即便把encoder的学习率调为0，也不影响模型损失下降，幸好我把不同模块的学习率分开了，不然我绝对看不出来。是LSTM的梯度截断，导致encoder无法训练，但不截断梯度又会报错。
    嗯，还有一个方案，由于memory有强残差连接，我可以先训练encoder和decoder。最后冻结他们两个的参数，单独去训练memory，幸好我还设计了一个没有memory组件的模型，原本是打算当成对照组的。
                                    5.11
                                    wqws
```
——
```
    好吧，即便当我使用没有memory的模型进行训练，encoder一样没有被训练。最后我统一了encoder，decoder的归一化，注意力，残差连接的先后顺序，都为Pre-Norm。并且让位置编码的标准差为1。encoder变得能训练了。
    我重新使用带有memory的模型，encoder也能够训练，看来并不是memory影响了encoder训练，这样就解决了这些问题。
    但现在模型的loss的下降速度极快，我难以准确控制模型的loss，模型在loss接近1时仍然会生成大量重复标点序列。我并不清楚这样的修改是否有用，虽然loss的下降比之前快了很多，
                                    5.12
                                    wqws
```